# MODULE 2: EXPERIMENT RUNNER (v4 - Multi-Model Support)
# ------------------------------------------------------
# Handles CLIP, SigLIP, and BLIP models based on model ID.
# Loads images generated by Module 1 across different sizes
# and performs analysis, plotting results against shape size.
# ------------------------------------------------------

import torch
import numpy as np
from PIL import Image
import os
import sys
from tqdm.notebook import tqdm # Or from tqdm import tqdm
import matplotlib.pyplot as plt
from scipy.spatial.distance import cosine as cosine_distance
import lpips # Still needed for visual distance

# --- Import model classes --- <<< MODIFIED HERE >>>
from transformers import (
    CLIPProcessor, CLIPModel,
    SiglipProcessor, SiglipModel,
    BlipProcessor, BlipModel, BlipVisionModel, # Add BLIP classes
    AutoProcessor, AutoModel # Generic loaders can sometimes work but explicit is safer
)
# --- End Import ---

import collections
import re
import yaml
import argparse

# --- Argument Parsing --- (Unchanged from your previous version)
parser = argparse.ArgumentParser(description="Run Vision Model Minimal Change Sensitivity Analysis")
# ... (keep argparse setup as you had it) ...
parser.add_argument('--clip_model', type=str, default=None, help='Vision Model ID (e.g., openai/clip-*, google/siglip-*, salesforce/blip-*)')
parser.add_argument('--lpips_net', type=str, default=None, help='Override LPIPS_NET_TYPE from config.yaml')
parser.add_argument('--output_dir', type=str, default=None, help='Override OUTPUT_DIR from config.yaml')
parser.add_argument('--config_file', type=str, default='config.yaml', help='Path to the YAML configuration file')
args = parser.parse_args()


# --- Configuration Loading ---
config = None
config_path = None
try:
    # Construct the full path to the config file
    script_dir = os.path.dirname(os.path.abspath(__file__)) if "__file__" in locals() else os.getcwd()
    config_path = os.path.join(script_dir, args.config_file) # Use path from arg

    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Configuration file not found at: {config_path}")

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
        if config is None:
            config = {} # Treat empty file as empty config
            print(f"Warning: Configuration file '{args.config_file}' is empty.")

except FileNotFoundError as e:
    print(f"Error: {e}")
    # If config file is essential even when args are provided, exit here.
    # If args can fully specify a run without config, maybe continue or use defaults.
    # For this setup, we likely need BASE_STIMULI_DIR etc., so we exit.
    sys.exit(1)
except yaml.YAMLError as e:
    print(f"Error parsing YAML file '{args.config_file}': {e}")
    sys.exit(1)
except Exception as e:
    print(f"An unexpected error occurred loading config: {e}")
    sys.exit(1)


# --- Determine Effective Configuration ---
# Start with defaults from config, then override with args if provided.
try:
    exp_config = config.get('Experiment', {})

    # Get base values from config (these act as defaults if args are not given)
    cfg_clip_model = exp_config.get('CLIP_MODEL_ID')
    cfg_lpips_net = exp_config.get('LPIPS_NET_TYPE')
    cfg_output_dir = exp_config.get('OUTPUT_DIR', 'results_default') # Provide a fallback default
    cfg_base_stimuli_dir = exp_config.get('BASE_STIMULI_DIR')
    cfg_shape_params = exp_config.get('ShapeSizeParams')

    # --- Override with args if they were passed ---
    VISION_MODEL_ID = args.clip_model if args.clip_model is not None else cfg_clip_model
    LPIPS_NET_TYPE = args.lpips_net if args.lpips_net is not None else cfg_lpips_net
    RUN_OUTPUT_DIR = args.output_dir if args.output_dir is not None else cfg_output_dir
    # --- These parameters are typically *not* overridden by args in this setup ---
    BASE_STIMULI_DIR = cfg_base_stimuli_dir
    shape_params = cfg_shape_params
    # --- End Override ---

    # --- Parameter Validation (Using effective values) ---
    missing = []
    if not VISION_MODEL_ID: missing.append("CLIP Model ID (config/arg)")
    if not LPIPS_NET_TYPE: missing.append("LPIPS Net Type (config/arg)")
    if not RUN_OUTPUT_DIR: missing.append("Output Directory (config/arg)")
    if not BASE_STIMULI_DIR: missing.append("BASE_STIMULI_DIR (config)")
    if not shape_params: missing.append("ShapeSizeParams (config)")
    # Add checks for nested shape_params keys if needed
    if missing:
        print(f"ERROR: Missing required configuration parameters: {', '.join(missing)}")
        print(f"       (Checked args and config file: '{config_path}')")
        sys.exit(1)

    # --- Generate SHAPE_SIZES_TESTED from shape_params ---
    SHAPE_SIZES_TESTED = None
    try:
        low = shape_params.get('lowest_size')
        high = shape_params.get('highest_size')
        interval = shape_params.get('interval')
        decimals = shape_params.get('rounding_decimals')
        if not all(isinstance(p, (int, float)) for p in [low, high, interval, decimals]):
            raise ValueError("Missing or non-numeric shape size parameters")

        SHAPE_SIZES_TESTED = np.round(np.arange(low, high, interval), int(decimals))
        if SHAPE_SIZES_TESTED.size == 0:
            raise ValueError(f"Shape size parameters resulted in empty range (low={low}, high={high}, interval={interval})")

    except Exception as e:
        print(f"Error processing ShapeSizeParams from config: {e}")
        sys.exit(1)

except KeyError as e:
    print(f"Error: Missing expected key in config file structure: {e}")
    sys.exit(1)
except Exception as e:
    print(f"An unexpected error occurred processing configuration: {e}")
    sys.exit(1)

# --- Technical Setup ---
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print(f"Using device: {device}")

# --- Generalized Feature Extraction Function --- <<< MODIFIED HERE >>>
def get_image_embedding(image_path, model, processor, model_type, device):
    """
    Loads image and extracts image embedding using the provided model and processor.
    Handles differences between CLIP, SigLIP, and BLIP.
    """
    try:
        image = Image.open(image_path).convert("RGB")
        # --- Preprocessing ---
        # Processors might handle inputs slightly differently.
        # BlipProcessor might require separate image/text handling if used for multimodal tasks,
        # but for image-only, passing `images=` should work.
        # Use pixel_values common key if possible
        inputs = processor(images=image, return_tensors="pt", padding=True).to(device)

        with torch.no_grad():
            # --- Feature Extraction ---
            if model_type == "CLIP" or model_type == "SigLIP":
                # Both CLIP and SigLIP have a convenient get_image_features method
                image_features = model.get_image_features(**inputs)
            elif model_type == "BLIP":
                # BLIP requires a bit more care.
                # Option 1: Try get_image_features if the BlipModel instance has it
                if hasattr(model, 'get_image_features'):
                    image_features = model.get_image_features(**inputs)
                    # BlipModel's get_image_features often returns last_hidden_state [batch, seq_len, hidden_dim]
                    # We usually need a pooled output [batch, hidden_dim] for comparison.
                    # Standard practice is often to take the embedding of the [CLS] token (index 0)
                    # or average pool the sequence dimension. Let's try CLS token.
                    if image_features.ndim == 3: # Check if sequence dimension exists
                        image_features = image_features[:, 0, :] # Take embedding of first token ([CLS])
                # Option 2: Use the vision model directly if BlipModel didn't work well
                elif hasattr(model, 'vision_model') and isinstance(model.vision_model, BlipVisionModel):
                    # Assumes inputs contains 'pixel_values' correctly preprocessed
                    vision_outputs = model.vision_model(pixel_values=inputs.get('pixel_values'))
                    # Pooler output is usually the desired embedding
                    image_features = vision_outputs.pooler_output
                # Option 3: If 'model' itself is BlipVisionModel
                elif isinstance(model, BlipVisionModel):
                    vision_outputs = model(pixel_values=inputs.get('pixel_values'))
                    image_features = vision_outputs.pooler_output
                else:
                    # Fallback if unsure how to get features from this specific BLIP model
                    raise NotImplementedError(f"Image feature extraction not implemented for this BLIP model variant: {type(model).__name__}")
            elif model_type == "google/vit-base-patch16-224":
                # Example of a specific model type that might need different handling
                # Assuming this is a ViT model, we can use the processor directly
                image_features = model(**inputs).last_hidden_state[:, 0, :] # CLS token
            else:
                raise ValueError(f"Unknown model_type '{model_type}' for feature extraction")

            # --- Normalization ---
            # Apply L2 normalization for consistency when using cosine distance
            if image_features is not None and isinstance(image_features, torch.Tensor):
                image_features = image_features / (image_features.norm(p=2, dim=-1, keepdim=True) + 1e-6) # Add epsilon for stability
            else:
                # Handle case where feature extraction failed
                print(f"Warning: image_features are None or not a Tensor for {image_path}. Skipping normalization.")
                return None # Return None if features couldn't be extracted

        return image_features.cpu().numpy().squeeze()

    except Exception as e:
        # Include model type in error for easier debugging
        print(f"Error processing {image_path} with {model_type} model ({VISION_MODEL_ID}): {e}")
        # Optionally raise e # Uncomment to stop execution on first error
        return None

# --- Helper Functions (get_clip_embedding, get_lpips_distance - unchanged from run_experiment_v2.py) ---
def get_clip_embedding(image_path, clip_model, clip_processor, device):
    """Loads image and extracts CLIP embedding."""
    try:
        image = Image.open(image_path).convert("RGB")
        inputs = clip_processor(images=image, return_tensors="pt", padding=True).to(device)
        with torch.no_grad():
            image_features = clip_model.get_image_features(**inputs)
        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
        return image_features.cpu().numpy().squeeze()
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
        return None

def get_lpips_distance(img_path1, img_path2, lpips_model, device):
    """Calculates LPIPS distance between two image files."""
    try:
        img1_tensor = lpips.im2tensor(lpips.load_image(img_path1)).to(device)
        img2_tensor = lpips.im2tensor(lpips.load_image(img_path2)).to(device)
        with torch.no_grad():
            distance = lpips_model(img1_tensor, img2_tensor).item()
        return distance
    except Exception as e:
        print(f"Error calculating LPIPS for {os.path.basename(img_path1)} vs {os.path.basename(img_path2)}: {e}")
        return None


# --- Main Experiment Script ---

if __name__ == "__main__":
    # --- Configuration and Parameter Printing --- (Uses VISION_MODEL_ID now)
    print("\n--- Module 2: Running Minimal Change Experiment (Size Sweep Analysis - Multi-Model) ---")
    print(f"--- Configuration Used ---")
    print(f"  Vision Model: {VISION_MODEL_ID}") # Changed label
    print(f"  LPIPS Net:    {LPIPS_NET_TYPE}")
    print(f"  Stimuli Base: {BASE_STIMULI_DIR}")
    print(f"  Output Dir:   {RUN_OUTPUT_DIR}")
    print(f"  Sizes Tested: {SHAPE_SIZES_TESTED}")
    print(f"--------------------------")

    # --- Ensure Output Directory Exists ---
    run_output_path_full = os.path.abspath(RUN_OUTPUT_DIR)
    os.makedirs(run_output_path_full, exist_ok=True)
    print(f"Results will be saved to: {run_output_path_full}")

    # --- 1. Load Models --- 
    print("Loading vision model and processor...")
    model = None
    processor = None
    model_type = "unknown" # To pass to helper function

    try:
        print(f"Loading model '{VISION_MODEL_ID}'...")
        # Determine model type based on ID string (simple substring check)
        # Make checks more specific if needed (e.g., check prefix)
        model_id_lower = VISION_MODEL_ID.lower()
        if "clip" in model_id_lower:
            print("Detected CLIP model type.")
            model_type = "CLIP"
            model = CLIPModel.from_pretrained(VISION_MODEL_ID).to(device)
            processor = CLIPProcessor.from_pretrained(VISION_MODEL_ID)
        elif "siglip" in model_id_lower:
            print("Detected SigLIP model type.")
            model_type = "SigLIP"
            model = SiglipModel.from_pretrained(VISION_MODEL_ID).to(device)
            processor = SiglipProcessor.from_pretrained(VISION_MODEL_ID)
        elif "blip" in model_id_lower:
            print("Detected BLIP model type.")
            model_type = "BLIP"
            # Using BlipModel as it often contains the vision tower and convenient methods
            # If issues arise, might need BlipVisionModel explicitly
            model = BlipModel.from_pretrained(VISION_MODEL_ID).to(device)
            processor = BlipProcessor.from_pretrained(VISION_MODEL_ID)
            # Test if processor requires separate image/text inputs later
            # processor = AutoProcessor.from_pretrained(VISION_MODEL_ID) # Alternative generic loader
        else:
            # Attempt generic loading (might work for CLIP/SigLIP compatible models)
            print(f"Warning: Unknown model type for '{VISION_MODEL_ID}'. Attempting generic AutoModel/AutoProcessor.")
            try:
                model_type = VISION_MODEL_ID
                model = AutoModel.from_pretrained(VISION_MODEL_ID).to(device)
                processor = AutoProcessor.from_pretrained(VISION_MODEL_ID)
            except Exception as auto_e:
                print(f"  Generic AutoLoad failed: {auto_e}")
                raise ValueError(f"Unsupported model type based on ID: {VISION_MODEL_ID}. Add specific loading logic.")

        print(f"Loaded {model_type} Model: {VISION_MODEL_ID}")

    except Exception as e:
        print(f"Error loading vision model '{VISION_MODEL_ID}': {e}. Exiting run.")
        sys.exit(1) # Use sys.exit

    try:
        lpips_model = lpips.LPIPS(net=LPIPS_NET_TYPE).to(device)
        lpips_model.eval()
        print(f"Loaded LPIPS Model: {LPIPS_NET_TYPE}")
    except Exception as e:
        print(f"Error loading LPIPS model: {e}. Exiting.")
        exit()
    print("Models loaded.")

    # --- 2. Process Results per Size ---
    results_by_size = collections.defaultdict(list) # Store lists of result dicts per size
    all_embeddings_cache = {} # Cache embeddings across all sizes

    script_dir = os.path.dirname(os.path.abspath(__file__)) if "__file__" in locals() else os.getcwd()
    base_stimuli_path_full = os.path.join(script_dir, BASE_STIMULI_DIR)

    # --- Outer loop for Shape Size ---
    for current_shape_size in SHAPE_SIZES_TESTED:
        size_str = f"{current_shape_size:.2f}"
        current_stimuli_dir = os.path.join(base_stimuli_path_full, f"size_{size_str}")
        # change / to \ if on Windows (optional, but can help with path issues)
        if os.name == 'nt':
            current_stimuli_dir = current_stimuli_dir.replace('/', '\\')
        print(f"\n--- Processing Size: {size_str} ---")

        if not os.path.isdir(current_stimuli_dir):
            print(f"Warning: Directory not found: {current_stimuli_dir}. Skipping size.")
            continue

        # --- Find files and pairs for THIS size ---
        all_files = [f for f in os.listdir(current_stimuli_dir) if f.endswith(".png")]
        if not all_files:
            print(f"Warning: No images found in {current_stimuli_dir}. Skipping size.")
            continue

        config_to_filepath = {}
        # Regex to parse filename: config_S{size}__{shape}_{shape}_{shape}.png
        pattern = re.compile(r"config_S(\d\.\d+)_+([a-z]+)_([a-z]+)_([a-z]+)\.png")
        for fname in all_files:
            match = pattern.match(fname)
            if match:
                size_in_fname = float(match.group(1))
                # Check if size matches current loop iteration (handle float precision)
                if abs(size_in_fname - current_shape_size) < 1e-5:
                    config_tuple = (match.group(2), match.group(3), match.group(4))
                    config_to_filepath[config_tuple] = os.path.join(current_stimuli_dir, fname)
            else:
                print(f"Warning: unrecognized filename format: {fname}, using as unknown.")
                # We use any png file that doesn't match the regex as a warning, but not an error
                config_tuple = (fname, "unknown", "unknown")
                config_to_filepath[config_tuple] = os.path.join(current_stimuli_dir, fname)


        # Find minimal pairs for THIS size
        minimal_pairs = []
        configs = list(config_to_filepath.keys())
        for i in range(len(configs)):
            for j in range(i + 1, len(configs)):
                conf1 = configs[i]
                conf2 = configs[j]
                diff_count = sum(1 for k in range(len(conf1)) if conf1[k] != conf2[k])
                if diff_count == 1:
                    fp1 = config_to_filepath[conf1]
                    fp2 = config_to_filepath[conf2]
                    minimal_pairs.append((fp1, fp2, conf1, conf2))

        if not minimal_pairs:
            print(f"Warning: No minimal pairs found for size {size_str}. Skipping analysis for this size.")
            continue

        print(f"Identified {len(minimal_pairs)} minimal pairs for size {size_str}.")

        # --- Extract embeddings for THIS size ---
        print("Extracting embeddings (using cache)...")
        current_size_paths = set()
        for fp1, fp2, _, _ in minimal_pairs:
            current_size_paths.add(fp1)
            current_size_paths.add(fp2)

        for img_path in tqdm(list(current_size_paths), desc=f"Embeddings {size_str}", leave=False):
            if img_path not in all_embeddings_cache:
                embedding = get_image_embedding(img_path, model, processor, model_type, device)
                if embedding is not None:
                    all_embeddings_cache[img_path] = embedding

        # --- Calculate Distances for THIS size ---
        print("Calculating distances...")
        size_results_list = []
        for fp1, fp2, conf1, conf2 in tqdm(minimal_pairs, desc=f"Distances {size_str}", leave=False):
            if fp1 in all_embeddings_cache and fp2 in all_embeddings_cache:
                E1 = all_embeddings_cache[fp1]
                E2 = all_embeddings_cache[fp2]
                d_image = cosine_distance(E1, E2)
                d_visual = get_lpips_distance(fp1, fp2, lpips_model, device)

                if d_visual is not None:
                    size_results_list.append({
                        'd_image': d_image,
                        'd_visual': d_visual,
                        'pair': (fp1, fp2),
                        'config_pair': (conf1, conf2)
                    })
            else:
                print(f"Warning: Skipping pair for size {size_str} due to missing embeddings: {os.path.basename(fp1)}, {os.path.basename(fp2)}")

        results_by_size[current_shape_size] = size_results_list
        print(f"Calculated distances for {len(size_results_list)} pairs for size {size_str}.")

    # --- End Size Loop ---

    # --- 3. Aggregate and Analyze Overall Results ---
    print("\n--- Aggregating Results Across Sizes ---")

    if not results_by_size:
        print("Error: No results were collected across any size. Exiting.")
        exit()

    avg_d_image_list = []
    avg_d_visual_list = []
    avg_ratio_list = []
    sizes_processed = sorted(results_by_size.keys())

    for size in sizes_processed:
        size_results = results_by_size[size]
        if not size_results:
            avg_d_image_list.append(np.nan)
            avg_d_visual_list.append(np.nan)
            avg_ratio_list.append(np.nan)
            print(f"Warning: No results for size {size}. Skipping.")
            continue

        d_images = [r['d_image'] for r in size_results]
        d_visuals = [r['d_visual'] for r in size_results]
        ratios = [r['d_image'] / r['d_visual'] for r in size_results if r['d_visual'] > 1e-6]

        avg_d_image_list.append(np.mean(d_images))
        avg_d_visual_list.append(np.mean(d_visuals))
        avg_ratio_list.append(np.mean(ratios) if ratios else np.nan)

    # --- 4. Plotting ---
    print("Generating plots...")
    plt.style.use('seaborn-v0_8-darkgrid') # Use a nice style

    fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)

    # Plot d_visual vs Size
    axs[0].plot(sizes_processed, avg_d_visual_list, marker='o', linestyle='-', color='blue')
    axs[0].set_title('Average Visual Distance (LPIPS) vs. Shape Size')
    axs[0].set_ylabel('Avg LPIPS Distance (d_visual)')
    axs[0].grid(True)

    # Plot d_image vs Size
    axs[1].plot(sizes_processed, avg_d_image_list, marker='s', linestyle='-', color='red')
    axs[1].set_title('Average Embedding Distance (Cosine) vs. Shape Size')
    axs[1].set_ylabel('Avg Cosine Distance (d_image)')
    axs[1].grid(True)

    # Plot R_swap vs Size
    axs[2].plot(sizes_processed, avg_ratio_list, marker='^', linestyle='-', color='green')
    axs[2].set_title('Average Sensitivity Ratio (R_swap = d_image/d_visual) vs. Shape Size')
    axs[2].set_ylabel('Avg Ratio (R_swap)')
    axs[2].set_xlabel('Shape Size (Blender Units)')
    axs[2].axhline(1.0, color='grey', linestyle='--', linewidth=0.8, label='Ratio = 1.0') # Add y=1 line
    axs[2].legend()
    axs[2].grid(True)


    plt.tight_layout()
    # --- Sanitize parameters for filename ---
    # Replace characters invalid for paths/filenames (like '/') with underscores
    safe_clip_name = VISION_MODEL_ID.replace('/', '_').replace('\\', '_')
    safe_lpips_name = LPIPS_NET_TYPE.replace('/', '_').replace('\\', '_')
    # Optional: Shorten or exclude stimuli dir from filename if it's too long
    # safe_stimuli_basename = os.path.basename(BASE_STIMULI_DIR).replace('.', '_')

    # --- Construct safer filename ---
    # Simplified: just use sanitized model and lpips type.
    # The RUN_OUTPUT_DIR (e.g., "results/specific_test_run" or from batch script) already provides context.
    plot_base_filename = f"size_sweep_analysis_{safe_clip_name}_{safe_lpips_name}.png"
    plot_filename = os.path.join(RUN_OUTPUT_DIR, plot_base_filename)

    # Ensure the directory exists *before* saving
    try:
        os.makedirs(RUN_OUTPUT_DIR, exist_ok=True) # Should exist, but safe to double-check
        plt.savefig(plot_filename)
        print(f"Saved analysis plots to: {plot_filename}")
    except Exception as e:
        print(f"ERROR saving plot '{plot_filename}': {e}")

    plt.close(fig) # Close the figure after saving to free up memory
    plt.show()

    # --- 5. Final Interpretation (Based on Plots) ---
    print("\n--- Final Interpretation based on Plots ---")
    # Describe the observed trends in the plots here based on visual inspection
    print("Analyze the generated plots ('size_sweep_analysis.png'):")
    print("1. How does Avg LPIPS Distance change with Shape Size? (Expected: Increase)")
    print("2. How does Avg Embedding Distance change with Shape Size? (Key question)")
    print("3. How does the Avg Sensitivity Ratio (R_swap) change with Shape Size?")
    print("   - Does it stay constant? Decrease? Increase?")
    print("   - Does it stay below, above, or cross the 1.0 line?")
    print("If R_swap consistently decreases or stays well below 1.0 as visual difference (size)")
    print("increases, it strengthens the argument for relative insensitivity / information loss")
    print("regarding single object identity changes for CLIP in this context.")


    print("\n--- Experiment Complete ---")