{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f785e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm # Or from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "import lpips # Still needed for visual distance\n",
    "\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    SiglipProcessor, SiglipModel,\n",
    "    BlipProcessor, BlipModel, BlipVisionModel, # Add BLIP classes\n",
    "    AutoProcessor, AutoModel # Generic loaders can sometimes work but explicit is safer\n",
    ")\n",
    "# --- End Import ---\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import yaml\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9bc6f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODEL_ID = \"openai/clip-vit-base-patch16\"\n",
    "# Siglip_MODEL_ID = \"google/siglip-base-patch16-512\"\n",
    "BLIP_MODEL_ID = \"Salesforce/blip-image-captioning-base\" \n",
    "ViT_MODEL_ID = \"google/owlvit-base-patch32\" # For ViT\n",
    "LPIPS_NET_TYPE = \"vgg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c088f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path, model, processor, model_type, device, VISION_MODEL_ID):\n",
    "    \"\"\"\n",
    "    Loads image and extracts image embedding using the provided model and processor.\n",
    "    Handles differences between CLIP, SigLIP, and BLIP.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # --- Preprocessing ---\n",
    "        # Processors might handle inputs slightly differently.\n",
    "        # BlipProcessor might require separate image/text handling if used for multimodal tasks,\n",
    "        # but for image-only, passing `images=` should work.\n",
    "        # Use pixel_values common key if possible\n",
    "        inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if model_type == \"CLIP\" or model_type == \"SigLIP\":\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "            elif model_type == \"BLIP\":\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "            elif model_type == \"google/vit-base-patch16-224\" or model_type == \"ViT\":\n",
    "                # Example of a specific model type that might need different handling\n",
    "                # Assuming this is a ViT model, we can use the processor directly\n",
    "                image_features = model(**inputs).last_hidden_state[:, 0, :] # CLS token\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_type '{model_type}' for feature extraction\")\n",
    "\n",
    "            # --- Normalization ---\n",
    "            # Apply L2 normalization for consistency when using cosine distance\n",
    "            if image_features is not None and isinstance(image_features, torch.Tensor):\n",
    "                image_features = image_features / (image_features.norm(p=2, dim=-1, keepdim=True) + 1e-6) # Add epsilon for stability\n",
    "            else:\n",
    "                # Handle case where feature extraction failed\n",
    "                print(f\"Warning: image_features are None or not a Tensor for {image_path}. Skipping normalization.\")\n",
    "                return None # Return None if features couldn't be extracted\n",
    "\n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Include model type in error for easier debugging\n",
    "        print(f\"Error processing {image_path} with {model_type} model ({VISION_MODEL_ID}): {e}\")\n",
    "        # Optionally raise e # Uncomment to stop execution on first error\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd2825f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09dd8e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55f6e326b414912814bf0cbf8fa34fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ene\\.cache\\huggingface\\hub\\models--google--owlvit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c69e2a9a1dd415d806f9536c83d9459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/613M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e7acad1c53456fa0e9897bce60c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0c75db2f2844a5b6c43e2513fc9361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5653c6f668014603ab4ee5821f192d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2b140559ae4f23a1168ce1492d96a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34824de172fd441b9b3735b349acf6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = AutoModel.from_pretrained(CLIP_MODEL_ID).to(device)\n",
    "clip_processor = AutoProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "\n",
    "# siglip_model = AutoModel.from_pretrained(Siglip_MODEL_ID).to(device)\n",
    "# siglip_processor = AutoProcessor.from_pretrained(Siglip_MODEL_ID)\n",
    "\n",
    "blip_model = AutoModel.from_pretrained(BLIP_MODEL_ID).to(device)\n",
    "blip_processor = AutoProcessor.from_pretrained(BLIP_MODEL_ID)\n",
    "\n",
    "ViT_model = AutoModel.from_pretrained(ViT_MODEL_ID).to(device)\n",
    "processor = AutoProcessor.from_pretrained(ViT_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "605326d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing test_images/3D_shape_sweep_only/size_0.10/config_S0.10__circle_circle_square.png with ViT model (google/owlvit-base-patch32): 'NoneType' object has no attribute 'size'\n",
      "CLIP embedding shape: (512,)\n",
      "BLIP embedding shape: (512,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print(\"SigLIP embedding shape:\", siglip_embedding.shape)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBLIP embedding shape:\u001b[39m\u001b[33m\"\u001b[39m, blip_embedding.shape)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mViT embedding shape:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mViT_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "image_path = \"test_images/3D_shape_sweep_only/size_0.10/config_S0.10__circle_circle_square.png\"\n",
    "clip_embedding = get_image_embedding(image_path, clip_model, clip_processor, \"CLIP\", device, CLIP_MODEL_ID)\n",
    "# siglip_embedding = get_image_embedding(image_path, siglip_model, siglip_processor, \"SigLIP\", device, Siglip_MODEL_ID)\n",
    "blip_embedding = get_image_embedding(image_path, blip_model, blip_processor, \"BLIP\", device, BLIP_MODEL_ID)\n",
    "ViT_embedding = get_image_embedding(image_path, ViT_model, processor, \"ViT\", device, ViT_MODEL_ID)\n",
    "print(\"CLIP embedding shape:\", clip_embedding.shape)\n",
    "# print(\"SigLIP embedding shape:\", siglip_embedding.shape)\n",
    "print(\"BLIP embedding shape:\", blip_embedding.shape)\n",
    "print(\"ViT embedding shape:\", ViT_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9aea38c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m inputs = processor(images=image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():    \n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     image_features = \u001b[43mViT_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\ve\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\ve\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\ve\\Lib\\site-packages\\transformers\\models\\owlvit\\modeling_owlvit.py:1212\u001b[39m, in \u001b[36mOwlViTModel.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding, return_base_image_embeds, return_dict)\u001b[39m\n\u001b[32m   1203\u001b[39m vision_outputs = \u001b[38;5;28mself\u001b[39m.vision_model(\n\u001b[32m   1204\u001b[39m     pixel_values=pixel_values,\n\u001b[32m   1205\u001b[39m     output_attentions=output_attentions,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1208\u001b[39m     return_dict=return_dict,\n\u001b[32m   1209\u001b[39m )\n\u001b[32m   1211\u001b[39m \u001b[38;5;66;03m# Get embeddings for all text queries in all batch samples\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1212\u001b[39m text_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m text_embeds = text_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1221\u001b[39m text_embeds = \u001b[38;5;28mself\u001b[39m.text_projection(text_embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\ve\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\ve\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\envs\\ve\\Lib\\site-packages\\transformers\\models\\owlvit\\modeling_owlvit.py:838\u001b[39m, in \u001b[36mOwlViTTextTransformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    833\u001b[39m output_hidden_states = (\n\u001b[32m    834\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    835\u001b[39m )\n\u001b[32m    836\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m input_shape = \u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m()\n\u001b[32m    839\u001b[39m input_ids = input_ids.view(-\u001b[32m1\u001b[39m, input_shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    840\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():    \n",
    "    image_features = ViT_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7776a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading VGG16 model...\n",
      "VGG16 Preprocessing Transforms:\n",
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "\n",
      "Getting embedding for: test_images/3D_shape_sweep_only/size_0.10/config_S0.10__circle_circle_square.png\n",
      "Successfully generated VGG16 Embedding.\n",
      "Output Embedding shape: (512,)\n",
      "First 5 values: [0.08766108 0.00056495 0.01977451 0.00235377 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_cnn_embedding_vgg16(image_path, feature_extractor, pooling_layer, transform, device):\n",
    "    \"\"\"\n",
    "    Calculates a 1D embedding vector for an image using a pre-trained VGG16\n",
    "    feature extractor and Global Average Pooling.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the image file.\n",
    "        feature_extractor (torch.nn.Module): The VGG16 features module.\n",
    "        pooling_layer (torch.nn.Module): The Global Average Pooling layer.\n",
    "        transform (torchvision.transforms.Compose): Preprocessing transforms.\n",
    "        device (torch.device): CPU or CUDA device.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D NumPy array representing the image embedding (shape (512,)),\n",
    "                    or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        # Apply transformations (resize, crop, normalize, etc.)\n",
    "        img_t = transform(img)\n",
    "        # Add batch dimension [C, H, W] -> [1, C, H, W]\n",
    "        batch_t = torch.unsqueeze(img_t, 0).to(device)\n",
    "\n",
    "        # --- Inference ---\n",
    "        # Set model to evaluation mode and disable gradients\n",
    "        feature_extractor.eval()\n",
    "        pooling_layer.eval()\n",
    "        with torch.no_grad():\n",
    "            # 1. Extract features from convolutional layers\n",
    "            # Output shape: [1, 512, H/32, W/32] (e.g., [1, 512, 7, 7] for 224x224 input)\n",
    "            features = feature_extractor(batch_t)\n",
    "\n",
    "            # 2. Apply Global Average Pooling\n",
    "            # Output shape: [1, 512, 1, 1]\n",
    "            pooled_features = pooling_layer(features)\n",
    "\n",
    "        # 3. Flatten the pooled features to get the embedding vector\n",
    "        # Reshape from [1, 512, 1, 1] to [1, 512] then flatten to (512,)\n",
    "        # Using .squeeze() removes dimensions of size 1\n",
    "        embedding = pooled_features.squeeze() # Shape: [512]\n",
    "\n",
    "        # Detach from GPU, move to CPU, convert to NumPy\n",
    "        embedding_np = embedding.detach().cpu().numpy()\n",
    "\n",
    "        # Optional: L2 Normalize (consistent with previous steps, though maybe less critical\n",
    "        # for some CNN embeddings depending on downstream use)\n",
    "        # norm = np.linalg.norm(embedding_np)\n",
    "        # if norm > 0:\n",
    "        #     embedding_np = embedding_np / norm\n",
    "\n",
    "        return embedding_np\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting VGG16 embedding for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Setup and Example Usage ---\n",
    "\n",
    "# Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load pre-trained VGG16 model and weights\n",
    "print(\"Loading VGG16 model...\")\n",
    "weights = models.VGG16_Weights.IMAGENET1K_V1 # Use recommended weights enum\n",
    "vgg_model = models.vgg16(weights=weights).to(device)\n",
    "vgg_model.eval() # Set to evaluation mode\n",
    "\n",
    "# 2. Isolate the feature extractor part (convolutional layers)\n",
    "# The output of this has 512 channels for VGG16\n",
    "feature_extractor = vgg_model.features\n",
    "\n",
    "# 3. Define the Global Average Pooling layer\n",
    "# Output size (1, 1) means pool each channel down to a single value\n",
    "pooling_layer = torch.nn.AdaptiveAvgPool2d((1, 1)).to(device)\n",
    "\n",
    "# 4. Get the correct preprocessing transforms for these weights\n",
    "preprocess_transform = weights.transforms()\n",
    "print(\"VGG16 Preprocessing Transforms:\")\n",
    "print(preprocess_transform)\n",
    "\n",
    "# --- Test with an example image path ---\n",
    "# Replace with a path to one of your PNG images\n",
    "# example_image_path = \"test_images/3D_shape_sweep_only/size_0.10/your_image.png\"\n",
    "example_image_path = image_path # Using the uploaded image for demo\n",
    "\n",
    "if Path(example_image_path).exists():\n",
    "    print(f\"\\nGetting embedding for: {example_image_path}\")\n",
    "    vgg_embedding_vector = get_cnn_embedding_vgg16(\n",
    "        example_image_path,\n",
    "        feature_extractor,\n",
    "        pooling_layer,\n",
    "        preprocess_transform,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    if vgg_embedding_vector is not None:\n",
    "        print(f\"Successfully generated VGG16 Embedding.\")\n",
    "        print(f\"Output Embedding shape: {vgg_embedding_vector.shape}\") # Should be (512,)\n",
    "        print(f\"First 5 values: {vgg_embedding_vector[:5]}\")\n",
    "\n",
    "        # Now you can use 'vgg_embedding_vector' in your visualization script\n",
    "        # You would add \"VGG16\" to your MODEL_INFO, MARKERS, etc.\n",
    "        # Remember its dimension is 512, so you'll still need the\n",
    "        # separate t-SNE approach if combining with 768d models.\n",
    "else:\n",
    "    print(f\"Example image path not found: {example_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d30381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# from sklearn.manifold import TSNE # Or PCA, UMAP\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# # Assuming PIL (Pillow) is used within get_image_embedding to load images\n",
    "\n",
    "# # --- Configuration ---\n",
    "# BASE_IMAGE_DIR = Path(\"test_images/3D_shape_sweep_only\")\n",
    "# OUTPUT_PLOT_DIR = Path(\"embedding_visualizations\")\n",
    "# OUTPUT_PLOT_DIR.mkdir(parents=True, exist_ok=True) # Create output dir if needed\n",
    "\n",
    "# # --- Assumed Variables (Make sure these are loaded/defined) ---\n",
    "# # Replace with your actual loaded models, processors, and IDs\n",
    "# # CLIP_MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
    "# # Siglip_MODEL_ID = \"google/siglip-so400m-patch14-384\" # Example matching 1152 dim\n",
    "# # BLIP_MODEL_ID = \"Salesforce/blip-itm-large-coco\" # Example matching 768 dim\n",
    "\n",
    "# # clip_model = ...\n",
    "# # clip_processor = ...\n",
    "# # siglip_model = ...\n",
    "# # siglip_processor = ...\n",
    "# # blip_model = ...\n",
    "# # blip_processor = ...\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MODEL_INFO = {\n",
    "#     \"CLIP\": {\"model\": clip_model, \"processor\": clip_processor, \"id\": CLIP_MODEL_ID},\n",
    "#     \"SigLIP\": {\"model\": siglip_model, \"processor\": siglip_processor, \"id\": Siglip_MODEL_ID},\n",
    "#     \"BLIP\": {\"model\": blip_model, \"processor\": blip_processor, \"id\": BLIP_MODEL_ID},\n",
    "# }\n",
    "\n",
    "# # Plotting config\n",
    "# COLORS = {\"CLIP\": \"red\", \"SigLIP\": \"blue\", \"BLIP\": \"green\"}\n",
    "# MARKERS = {\"CLIP\": \"o\", \"SigLIP\": \"o\", \"BLIP\": \"o\"} # Optional: different markers\n",
    "# TSNE_PERPLEXITY = 15 # Adjust based on number of points per plot (usually 5-50)\n",
    "# TSNE_N_ITER = 300  # Number of iterations for t-SNE\n",
    "\n",
    "# # --- Helper Function (Assuming you have this) ---\n",
    "# # Define or import your get_image_embedding function here.\n",
    "# # It should return a numpy array or torch tensor.\n",
    "# # Example structure:\n",
    "# # def get_image_embedding(image_path, model, processor, model_name, device, model_id):\n",
    "# #     # ... (load image with PIL)\n",
    "# #     # ... (process image with processor)\n",
    "# #     # ... (run model inference)\n",
    "# #     # ... (extract embedding, e.g., model.get_image_features(**inputs))\n",
    "# #     # Return embedding as numpy array or tensor\n",
    "# #     # Example: return features.detach().cpu().numpy()\n",
    "# #     pass # Replace with your actual function\n",
    "\n",
    "\n",
    "# # --- Main Visualization Loop ---\n",
    "# print(f\"Starting visualization process. Plots will be saved to: {OUTPUT_PLOT_DIR}\")\n",
    "\n",
    "# # Iterate through each size_* directory\n",
    "# for size_dir in sorted(BASE_IMAGE_DIR.glob(\"size_*\")):\n",
    "#     if not size_dir.is_dir():\n",
    "#         continue\n",
    "\n",
    "#     print(f\"\\nProcessing directory: {size_dir.name}\")\n",
    "#     image_paths = list(size_dir.glob(\"*.png\"))\n",
    "\n",
    "#     if not image_paths:\n",
    "#         print(f\"  No PNG images found in {size_dir.name}. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"  Found {len(image_paths)} PNG images.\")\n",
    "\n",
    "#     # Store embeddings separately for each model\n",
    "#     model_embeddings_dict = {model_name: [] for model_name in MODEL_INFO}\n",
    "\n",
    "#     # Generate embeddings for all images in this directory for all models\n",
    "#     for i, img_path in enumerate(image_paths):\n",
    "#         for model_name, info in MODEL_INFO.items():\n",
    "#             try:\n",
    "#                 embedding = get_image_embedding(\n",
    "#                     img_path, info[\"model\"], info[\"processor\"], model_name, DEVICE, info[\"id\"]\n",
    "#                 )\n",
    "\n",
    "#                 # Ensure embedding is a flat numpy array\n",
    "#                 if isinstance(embedding, torch.Tensor):\n",
    "#                     embedding = embedding.detach().cpu().numpy()\n",
    "#                 embedding = embedding.flatten()\n",
    "\n",
    "#                 # L2 Normalize embeddings\n",
    "#                 norm = np.linalg.norm(embedding)\n",
    "#                 if norm > 0:\n",
    "#                     normalized_embedding = embedding / norm\n",
    "#                 else:\n",
    "#                     normalized_embedding = embedding # Avoid division by zero\n",
    "\n",
    "#                 # Append to the correct model's list\n",
    "#                 model_embeddings_dict[model_name].append(normalized_embedding)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  Error processing {img_path.name} with {model_name}: {e}\")\n",
    "\n",
    "\n",
    "#     # --- Dimensionality Reduction (Per Model) ---\n",
    "#     combined_2d_embeddings = []\n",
    "#     combined_labels = []\n",
    "\n",
    "#     for model_name, embeddings_list in model_embeddings_dict.items():\n",
    "#         if not embeddings_list:\n",
    "#             print(f\"  No embeddings generated for {model_name} in {size_dir.name}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         embeddings_array = np.array(embeddings_list)\n",
    "#         n_samples = embeddings_array.shape[0]\n",
    "#         print(f\"  Generated {n_samples} embeddings for {model_name} (dim={embeddings_array.shape[1]}).\")\n",
    "\n",
    "#         # Check conditions for t-SNE\n",
    "#         effective_perplexity = min(TSNE_PERPLEXITY, max(1, n_samples - 1))\n",
    "#         if n_samples <= 1:\n",
    "#             print(f\"  Only {n_samples} {model_name} embedding(s). Cannot run t-SNE. Skipping.\")\n",
    "#             continue\n",
    "#         if n_samples <= effective_perplexity:\n",
    "#             print(f\"  Adjusting perplexity for {model_name} from {TSNE_PERPLEXITY} to {max(1, n_samples - 1)}.\")\n",
    "#             effective_perplexity = max(1, n_samples - 1)\n",
    "\n",
    "#         print(f\"  Running t-SNE for {model_name} (perplexity={effective_perplexity}, n_iter={TSNE_N_ITER})...\")\n",
    "#         tsne = TSNE(\n",
    "#             n_components=2,\n",
    "#             random_state=42, # for reproducibility\n",
    "#             perplexity=effective_perplexity,\n",
    "#             n_iter=TSNE_N_ITER,\n",
    "#             init='pca', # PCA initialization is often more stable\n",
    "#             learning_rate='auto' # Recommended setting\n",
    "#         )\n",
    "#         try:\n",
    "#             # Run t-SNE on this model's embeddings only\n",
    "#             embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "#             # Append the 2D results and corresponding labels to combined lists\n",
    "#             combined_2d_embeddings.extend(embeddings_2d.tolist())\n",
    "#             combined_labels.extend([model_name] * n_samples)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  Error during t-SNE for {model_name} in {size_dir.name}: {e}. Skipping.\")\n",
    "\n",
    "\n",
    "#     # --- Plotting (Combined 2D Embeddings) ---\n",
    "#     if not combined_2d_embeddings:\n",
    "#         print(f\"  No 2D embeddings to plot for {size_dir.name}. Skipping plot.\")\n",
    "#         continue\n",
    "\n",
    "#     combined_2d_embeddings_array = np.array(combined_2d_embeddings)\n",
    "#     print(f\"  Generating combined plot for {len(combined_labels)} points...\")\n",
    "\n",
    "#     plt.figure(figsize=(12, 10))\n",
    "#     for model_name in MODEL_INFO.keys():\n",
    "#         # Find indices corresponding to the current model in the combined lists\n",
    "#         indices = [i for i, label in enumerate(combined_labels) if label == model_name]\n",
    "#         if indices: # Only plot if there are points for this model\n",
    "#             plt.scatter(\n",
    "#                 combined_2d_embeddings_array[indices, 0],\n",
    "#                 combined_2d_embeddings_array[indices, 1],\n",
    "#                 c=COLORS[model_name],\n",
    "#                 label=model_name,\n",
    "#                 marker=MARKERS[model_name], # Use different markers\n",
    "#                 alpha=0.7, # Adjust transparency\n",
    "#                 s=50 # Adjust marker size\n",
    "#             )\n",
    "\n",
    "#     plt.title(f\"t-SNE Visualization of Image Embeddings ({size_dir.name})\", fontsize=16)\n",
    "#     plt.xlabel(\"t-SNE Component 1\", fontsize=12)\n",
    "#     plt.ylabel(\"t-SNE Component 2\", fontsize=12)\n",
    "#     plt.legend(title=\"Model\", fontsize=10)\n",
    "#     plt.grid(True, linestyle='--', alpha=0.5)\n",
    "#     plt.tight_layout() # Adjust layout\n",
    "\n",
    "#     # Save the plot\n",
    "#     plot_filename = OUTPUT_PLOT_DIR / f\"{size_dir.name}_embedding_tsne.png\"\n",
    "#     plt.savefig(plot_filename, dpi=150) # Save with higher resolution\n",
    "#     print(f\"  Plot saved to {plot_filename}\")\n",
    "#     plt.close() # Close the figure to release memory\n",
    "\n",
    "# print(\"\\nVisualization process complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ceb14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting embeddings for ALL models across all sizes...\n",
      "  Processing directory: size_0.10\n",
      "  Processing directory: size_0.15\n",
      "  Processing directory: size_0.20\n",
      "  Processing directory: size_0.25\n",
      "  Processing directory: size_0.30\n",
      "  Processing directory: size_0.35\n",
      "  Processing directory: size_0.40\n",
      "  Processing directory: size_0.45\n",
      "  Processing directory: size_0.50\n",
      "  Processing directory: size_0.55\n",
      "  Processing directory: size_0.60\n",
      "  Processing directory: size_0.65\n",
      "  Processing directory: size_0.70\n",
      "  Processing directory: size_0.75\n",
      "  Processing directory: size_0.80\n",
      "  Processing directory: size_0.85\n",
      "  Processing directory: size_0.90\n",
      "  Processing directory: size_0.95\n",
      "  Processing directory: size_1.00\n",
      "  Processing directory: size_1.05\n",
      "  Processing directory: size_1.10\n",
      "  Processing directory: size_1.15\n",
      "  Processing directory: size_1.20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm # For color mapping for sizes\n",
    "import torch\n",
    "# Assuming PIL is used within get_image_embedding\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_IMAGE_DIR = Path(\"test_images/3D_shape_sweep_only\")\n",
    "OUTPUT_PLOT_DIR = Path(\"embedding_visualizations/Total_Combined\")\n",
    "OUTPUT_PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Embedding dimensions (as provided by user)\n",
    "MODEL_DIMS = {\n",
    "    \"CLIP\": 768,\n",
    "    \"SigLIP\": 768,\n",
    "    \"BLIP\": 512,\n",
    "    \"ViT\": 768, \n",
    "}\n",
    "\n",
    "# --- Assumed Variables (Make sure these are loaded/defined) ---\n",
    "# CLIP_MODEL_ID = ... Siglip_MODEL_ID = ... BLIP_MODEL_ID = ...\n",
    "# clip_model = ... clip_processor = ...\n",
    "# siglip_model = ... siglip_processor = ...\n",
    "# blip_model = ... blip_processor = ...\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_INFO = {\n",
    "    \"CLIP\": {\"model\": clip_model, \"processor\": clip_processor, \"id\": CLIP_MODEL_ID, \"dim\": MODEL_DIMS[\"CLIP\"]},\n",
    "    \"SigLIP\": {\"model\": siglip_model, \"processor\": siglip_processor, \"id\": Siglip_MODEL_ID, \"dim\": MODEL_DIMS[\"SigLIP\"]},\n",
    "    #\"BLIP\": {\"model\": blip_model, \"processor\": blip_processor, \"id\": BLIP_MODEL_ID, \"dim\": MODEL_DIMS[\"BLIP\"]},\n",
    "    \"ViT\": {\"model\": ViT_model, \"processor\": processor, \"id\": ViT_MODEL_ID, \"dim\": MODEL_DIMS[\"ViT\"]},\n",
    "}\n",
    "\n",
    "# Define markers for models\n",
    "MARKERS = {\"CLIP\": \"o\", \"SigLIP\": \"s\",  \"ViT\": \"D\"} #\"BLIP\": \"^\",\n",
    "\n",
    "# Output filename\n",
    "OUTPUT_PLOT_FILE = OUTPUT_PLOT_DIR / \"all_models_by_size_color_marker_tsne.png\"\n",
    "\n",
    "# t-SNE config\n",
    "TSNE_PERPLEXITY = 30\n",
    "TSNE_N_ITER = 350\n",
    "\n",
    "# --- Helper Function (Assuming you have this) ---\n",
    "# Define or import your get_image_embedding function here.\n",
    "# def get_image_embedding(image_path, model, processor, model_name, device, model_id):\n",
    "#     # ... (load image, process, inference) ...\n",
    "#     # Return embedding as numpy array or tensor\n",
    "#     pass # Replace with your actual function\n",
    "\n",
    "\n",
    "# --- Data Collection (Grouped by Model, Across All Sizes) ---\n",
    "print(\"Collecting embeddings for ALL models across all sizes...\")\n",
    "model_embeddings_dict = {model_name: [] for model_name in MODEL_INFO}\n",
    "# Store corresponding size labels for each embedding within each model's list\n",
    "model_size_labels_dict = {model_name: [] for model_name in MODEL_INFO}\n",
    "unique_size_labels = sorted([d.name for d in BASE_IMAGE_DIR.glob(\"size_*\") if d.is_dir()])\n",
    "if not unique_size_labels:\n",
    "    print(\"Error: No 'size_*' directories found in BASE_IMAGE_DIR.\")\n",
    "    exit()\n",
    "\n",
    "for size_label in unique_size_labels:\n",
    "    size_dir = BASE_IMAGE_DIR / size_label\n",
    "    print(f\"  Processing directory: {size_label}\")\n",
    "    image_paths = list(size_dir.glob(\"*.png\"))\n",
    "\n",
    "    if not image_paths:\n",
    "        print(f\"    No PNG images found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        for model_name, info in MODEL_INFO.items():\n",
    "            try:\n",
    "                embedding = get_image_embedding(\n",
    "                    img_path, info[\"model\"], info[\"processor\"], model_name, DEVICE, info[\"id\"]\n",
    "                )\n",
    "\n",
    "                if isinstance(embedding, torch.Tensor):\n",
    "                    embedding = embedding.detach().cpu().numpy()\n",
    "                embedding = embedding.flatten()\n",
    "\n",
    "                # Validate dimension\n",
    "                expected_dim = info['dim']\n",
    "                if embedding.shape[0] != expected_dim:\n",
    "                    print(f\"    Warning: Embedding dim mismatch for {model_name} ({img_path.name}). Expected {expected_dim}, got {embedding.shape[0]}. Skipping this embedding.\")\n",
    "                    continue\n",
    "\n",
    "                model_embeddings_dict[model_name].append(embedding)\n",
    "                model_size_labels_dict[model_name].append(size_label)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {img_path.name} with {model_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f1a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running t-SNE separately for each model's collected embeddings...\n",
      "  Processing CLIP: 552 samples, Dim=768\n",
      "    Running t-SNE for CLIP (perplexity=30, n_iter=350)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\anaconda\\envs\\ve\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda\\envs\\ve\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda\\envs\\ve\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"d:\\anaconda\\envs\\ve\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    t-SNE complete for CLIP.\n",
      "  Processing SigLIP: 552 samples, Dim=768\n",
      "    Running t-SNE for SigLIP (perplexity=30, n_iter=350)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    t-SNE complete for SigLIP.\n",
      "  Processing BLIP: 552 samples, Dim=512\n",
      "    Running t-SNE for BLIP (perplexity=30, n_iter=350)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    t-SNE complete for BLIP.\n",
      "  Processing ViT: 552 samples, Dim=768\n",
      "    Running t-SNE for ViT (perplexity=30, n_iter=350)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    t-SNE complete for ViT.\n"
     ]
    }
   ],
   "source": [
    "# --- Dimensionality Reduction (Run t-SNE Separately for Each Model) ---\n",
    "combined_2d_embeddings = []\n",
    "combined_model_labels_for_marker = [] # For marker shape\n",
    "combined_size_labels_for_color = []   # For color\n",
    "\n",
    "print(\"\\nRunning t-SNE separately for each model's collected embeddings...\")\n",
    "for model_name, embeddings_list in model_embeddings_dict.items():\n",
    "    if not embeddings_list:\n",
    "        print(f\"  No embeddings collected for {model_name}. Skipping t-SNE.\")\n",
    "        continue\n",
    "\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    n_samples, emb_dim = embeddings_array.shape\n",
    "    print(f\"  Processing {model_name}: {n_samples} samples, Dim={emb_dim}\")\n",
    "\n",
    "    # Check conditions for t-SNE\n",
    "    effective_perplexity = min(TSNE_PERPLEXITY, max(1, n_samples - 1))\n",
    "    if n_samples <= 1:\n",
    "        print(f\"    Only {n_samples} embedding(s). Cannot run t-SNE. Skipping.\")\n",
    "        continue\n",
    "    if n_samples <= effective_perplexity:\n",
    "        print(f\"    Adjusting perplexity for {model_name} from {TSNE_PERPLEXITY} to {max(1, n_samples - 1)}.\")\n",
    "        effective_perplexity = max(1, n_samples - 1)\n",
    "\n",
    "    print(f\"    Running t-SNE for {model_name} (perplexity={effective_perplexity}, n_iter={TSNE_N_ITER})...\")\n",
    "    tsne = TSNE(\n",
    "        n_components=2, random_state=42, perplexity=effective_perplexity,\n",
    "        n_iter=TSNE_N_ITER, init='pca', learning_rate='auto', n_jobs=-1\n",
    "    )\n",
    "    try:\n",
    "        embeddyings_2d = tsne.fit_transform(embeddings_array)\n",
    "        combined_2d_embeddings.extend(embeddings_2d.tolist())\n",
    "        combined_model_labels_for_marker.extend([model_name] * n_samples)\n",
    "        # Get the corresponding size labels that were stored earlier\n",
    "        combined_size_labels_for_color.extend(model_size_labels_dict[model_name])\n",
    "        print(f\"    t-SNE complete for {model_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Error during t-SNE for {model_name}: {e}. Skipping.\")\n",
    "\n",
    "\n",
    "# --- Plotting (Single Plot, Marker=Model, Color=Size) ---\n",
    "if not combined_2d_embeddings:\n",
    "    print(\"\\nNo 2D embeddings were generated from any model. Cannot create plot.\")\n",
    "    exit()\n",
    "\n",
    "combined_2d_embeddings_array = np.array(combined_2d_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0448543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating combined plot for 2208 points...\n",
      "\n",
      "Plot saved to embedding_visualizations\\Total_Combined\\all_models_by_size_color_marker_tsne.png\n",
      "\n",
      "Visualization process complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nGenerating combined plot for {len(combined_model_labels_for_marker)} points...\")\n",
    "\n",
    "plt.figure(figsize=(18, 15)) # Large figure\n",
    "\n",
    "# Create color map for SIZES\n",
    "num_sizes = len(unique_size_labels)\n",
    "# Use a visually distinct colormap like tab20 if num_sizes <= 20, otherwise viridis/plasma\n",
    "color_map = cm.tab20(np.linspace(0, 1, num_sizes)) if num_sizes <= 20 else cm.viridis(np.linspace(0, 1, num_sizes))\n",
    "size_to_color = {size_label: color_map[i] for i, size_label in enumerate(unique_size_labels)}\n",
    "\n",
    "# Plot all points, assigning marker and color individually\n",
    "# This is simpler than grouping if colors/markers are assigned per point anyway\n",
    "point_colors = [size_to_color[sz_label] for sz_label in combined_size_labels_for_color]\n",
    "point_markers = [MARKERS.get(m_label, 'x') for m_label in combined_model_labels_for_marker] # List of markers for each point\n",
    "\n",
    "# Need to plot each marker type separately to build the legend correctly\n",
    "legend_handles_models = []\n",
    "unique_models_plotted = sorted(list(set(combined_model_labels_for_marker)))\n",
    "\n",
    "for model_name in unique_models_plotted:\n",
    "    model_indices = [i for i, m_label in enumerate(combined_model_labels_for_marker) if m_label == model_name]\n",
    "    model_marker = MARKERS.get(model_name, 'x')\n",
    "\n",
    "    # Extract coordinates and colors for points belonging to this model\n",
    "    model_coords = combined_2d_embeddings_array[model_indices]\n",
    "    model_point_colors = [point_colors[i] for i in model_indices]\n",
    "\n",
    "    plt.scatter(\n",
    "        model_coords[:, 0],\n",
    "        model_coords[:, 1],\n",
    "        c=model_point_colors, # Color by size\n",
    "        marker=model_marker,  # Marker by model\n",
    "        label=model_name,     # Label only used for legend handle creation below\n",
    "        alpha=0.6,\n",
    "        s=40\n",
    "    )\n",
    "    # Create a representative handle for the model legend (marker type)\n",
    "    legend_handles_models.append(plt.Line2D([0], [0], marker=model_marker, color='grey', label=model_name, linestyle='', markersize=8))\n",
    "\n",
    "\n",
    "plt.title(f\"t-SNE Visualization (Marker=Model, Color=Size)\", fontsize=18)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=14)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=14)\n",
    "\n",
    "# Create legends - one for markers (models), one for colors (sizes)\n",
    "# Legend 1: Models (Markers)\n",
    "legend1 = plt.legend(handles=legend_handles_models, title=\"Model\", loc='upper left', bbox_to_anchor=(1.04, 1), fontsize=10)\n",
    "plt.gca().add_artist(legend1) # Add the first legend manually\n",
    "\n",
    "# Legend 2: Sizes (Colors) - Create dummy scatter points for the legend\n",
    "size_legend_handles = [plt.scatter([],[], color=size_to_color[sz_label], label=sz_label) for sz_label in unique_size_labels]\n",
    "plt.legend(handles=size_legend_handles, title=\"Size\", loc='center left', bbox_to_anchor=(1.04, 0.5), fontsize=9)\n",
    "\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "# Adjust layout AFTER legends are created\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout for external legends\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(OUTPUT_PLOT_FILE, dpi=200, bbox_inches='tight') # Use bbox_inches='tight' to include external legend\n",
    "print(f\"\\nPlot saved to {OUTPUT_PLOT_FILE}\")\n",
    "plt.close() # Close the figure to release memory\n",
    "\n",
    "print(\"\\nVisualization process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b07670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3e710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb548d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47eec0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting embeddings for CLIP, SigLIP across all sizes...\n",
      "  Processing directory: size_0.10\n",
      "  Processing directory: size_0.15\n",
      "  Processing directory: size_0.20\n",
      "  Processing directory: size_0.25\n",
      "  Processing directory: size_0.30\n",
      "  Processing directory: size_0.35\n",
      "  Processing directory: size_0.40\n",
      "  Processing directory: size_0.45\n",
      "  Processing directory: size_0.50\n",
      "  Processing directory: size_0.55\n",
      "  Processing directory: size_0.60\n",
      "  Processing directory: size_0.65\n",
      "  Processing directory: size_0.70\n",
      "  Processing directory: size_0.75\n",
      "  Processing directory: size_0.80\n",
      "  Processing directory: size_0.85\n",
      "  Processing directory: size_0.90\n",
      "  Processing directory: size_0.95\n",
      "  Processing directory: size_1.00\n",
      "  Processing directory: size_1.05\n",
      "  Processing directory: size_1.10\n",
      "  Processing directory: size_1.15\n",
      "  Processing directory: size_1.20\n",
      "\n",
      "Collected 1656 total embeddings (dimension: 768).\n",
      "Running t-SNE on the combined dataset (perplexity=30, n_iter=350)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ve\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  t-SNE calculation complete.\n",
      "Generating plot...\n",
      "\n",
      "Plot saved to embedding_visualizations\\CLIP_SigLIP_combined_tsne_marker_model_color_size.png\n",
      "\n",
      "Visualization process complete.\n"
     ]
    }
   ],
   "source": [
    "# All embedding\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "# Assuming PIL is used within get_image_embedding\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_IMAGE_DIR = Path(\"test_images/3D_shape_sweep_only\")\n",
    "OUTPUT_PLOT_DIR = Path(\"embedding_visualizations\")\n",
    "OUTPUT_PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Models to include (REMOVE BLIP)\n",
    "MODELS_TO_INCLUDE = [\"CLIP\", \"SigLIP\"]\n",
    "MODEL_DIMS = { \"CLIP\": 768, \"SigLIP\": 768, \"ViT\": 768 } # BLIP removed\n",
    "\n",
    "# --- Assumed Variables (Make sure these are loaded/defined) ---\n",
    "# CLIP_MODEL_ID = ... Siglip_MODEL_ID = ...\n",
    "# clip_model = ... clip_processor = ...\n",
    "# siglip_model = ... siglip_processor = ...\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Filter MODEL_INFO based on MODELS_TO_INCLUDE\n",
    "MODEL_INFO_FILTERED = {\n",
    "    \"CLIP\": {\"model\": clip_model, \"processor\": clip_processor, \"id\": CLIP_MODEL_ID, \"dim\": MODEL_DIMS[\"CLIP\"]},\n",
    "    \"SigLIP\": {\"model\": siglip_model, \"processor\": siglip_processor, \"id\": Siglip_MODEL_ID, \"dim\": MODEL_DIMS[\"SigLIP\"]},\n",
    "    \"ViT\": {\"model\": ViT_model, \"processor\": processor, \"id\": ViT_MODEL_ID, \"dim\": MODEL_DIMS[\"ViT\"]},\n",
    "    # BLIP entry is removed\n",
    "}\n",
    "\n",
    "# Define markers for models\n",
    "MARKERS = {\"CLIP\": \"o\", \"SigLIP\": \"s\", \"ViT\": \"D\"} # BLIP removed\n",
    "\n",
    "# Output filename\n",
    "OUTPUT_PLOT_FILE = OUTPUT_PLOT_DIR / \"CLIP_SigLIP_combined_tsne_marker_model_color_size.png\"\n",
    "\n",
    "# t-SNE config\n",
    "TSNE_PERPLEXITY = 30\n",
    "TSNE_N_ITER = 350\n",
    "\n",
    "# --- Helper Function (Assuming you have this) ---\n",
    "# Define or import your get_image_embedding function here.\n",
    "# def get_image_embedding(image_path, model, processor, model_name, device, model_id):\n",
    "#     # ... (load image, process, inference) ...\n",
    "#     # Return embedding as numpy array or tensor\n",
    "#     pass # Replace with your actual function\n",
    "\n",
    "\n",
    "# --- Data Collection (CLIP & SigLIP only) ---\n",
    "print(f\"Collecting embeddings for {', '.join(MODELS_TO_INCLUDE)} across all sizes...\")\n",
    "all_embeddings_list = []\n",
    "all_model_labels = [] # To determine marker shape\n",
    "all_size_labels = []  # To determine color\n",
    "unique_size_labels = sorted([d.name for d in BASE_IMAGE_DIR.glob(\"size_*\") if d.is_dir()])\n",
    "if not unique_size_labels:\n",
    "    print(\"Error: No 'size_*' directories found.\")\n",
    "    exit()\n",
    "\n",
    "for size_label in unique_size_labels:\n",
    "    size_dir = BASE_IMAGE_DIR / size_label\n",
    "    print(f\"  Processing directory: {size_label}\")\n",
    "    image_paths = list(size_dir.glob(\"*.png\"))\n",
    "\n",
    "    if not image_paths:\n",
    "        print(f\"    No PNG images found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        # Loop only through the models we want to include\n",
    "        for model_name, info in MODEL_INFO_FILTERED.items():\n",
    "            try:\n",
    "                embedding = get_image_embedding(\n",
    "                    img_path, info[\"model\"], info[\"processor\"], model_name, DEVICE, info[\"id\"]\n",
    "                )\n",
    "\n",
    "                if isinstance(embedding, torch.Tensor):\n",
    "                    embedding = embedding.detach().cpu().numpy()\n",
    "                embedding = embedding.flatten()\n",
    "\n",
    "                # Validate dimension\n",
    "                expected_dim = info['dim']\n",
    "                if embedding.shape[0] != expected_dim:\n",
    "                    print(f\"    Warning: Embedding dim mismatch for {model_name} ({img_path.name}). Expected {expected_dim}, got {embedding.shape[0]}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                all_embeddings_list.append(embedding)\n",
    "                all_model_labels.append(model_name) # Store model label\n",
    "                all_size_labels.append(size_label)   # Store size label\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing {img_path.name} with {model_name}: {e}\")\n",
    "\n",
    "\n",
    "# --- Single Dimensionality Reduction (on Combined 768d Data) ---\n",
    "if not all_embeddings_list:\n",
    "    print(\"\\nNo embeddings were collected. Cannot proceed.\")\n",
    "    exit()\n",
    "\n",
    "embeddings_array = np.array(all_embeddings_list)\n",
    "n_samples, emb_dim = embeddings_array.shape\n",
    "# Should always be 768 now if validation worked\n",
    "print(f\"\\nCollected {n_samples} total embeddings (dimension: {emb_dim}).\")\n",
    "\n",
    "print(f\"Running t-SNE on the combined dataset (perplexity={TSNE_PERPLEXITY}, n_iter={TSNE_N_ITER})...\")\n",
    "# Adjust perplexity if number of samples is too small relative to default\n",
    "effective_perplexity = min(TSNE_PERPLEXITY, max(1, n_samples - 1))\n",
    "if n_samples <= effective_perplexity:\n",
    "    print(f\"  Warning: Number of samples ({n_samples}) is less than or equal to perplexity ({effective_perplexity}). Adjusting perplexity.\")\n",
    "    effective_perplexity = max(1, n_samples - 1)\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2, random_state=42, perplexity=effective_perplexity,\n",
    "    n_iter=TSNE_N_ITER, init='pca', learning_rate='auto', n_jobs=-1\n",
    ")\n",
    "try:\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "    print(\"  t-SNE calculation complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during t-SNE: {e}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Plotting (Single Plot, Marker=Model, Color=Size) ---\n",
    "print(\"Generating plot...\")\n",
    "plt.figure(figsize=(18, 15))\n",
    "\n",
    "# Create color map for SIZES\n",
    "num_sizes = len(unique_size_labels)\n",
    "color_map = cm.tab20(np.linspace(0, 1, num_sizes)) if num_sizes <= 20 else cm.viridis(np.linspace(0, 1, num_sizes))\n",
    "size_to_color = {size_label: color_map[i] for i, size_label in enumerate(unique_size_labels)}\n",
    "\n",
    "# --- Plotting Logic ---\n",
    "# We need to plot each marker type separately to build the legend correctly\n",
    "legend_handles_models = []\n",
    "unique_models_plotted = sorted(list(set(all_model_labels))) # Will be ['CLIP', 'SigLIP']\n",
    "\n",
    "for model_name in unique_models_plotted:\n",
    "    # Find indices for points belonging to this model\n",
    "    model_indices = [i for i, m_label in enumerate(all_model_labels) if m_label == model_name]\n",
    "    model_marker = MARKERS.get(model_name, 'x') # Get marker for this model\n",
    "\n",
    "    # Extract coordinates and the corresponding size labels for these points\n",
    "    model_coords = embeddings_2d[model_indices]\n",
    "    model_point_size_labels = [all_size_labels[i] for i in model_indices]\n",
    "\n",
    "    # Determine the color for each point based on its size label\n",
    "    model_point_colors = [size_to_color[sz_label] for sz_label in model_point_size_labels]\n",
    "\n",
    "    # Scatter plot for this model's points\n",
    "    plt.scatter(\n",
    "        model_coords[:, 0],\n",
    "        model_coords[:, 1],\n",
    "        c=model_point_colors, # Color determined by size\n",
    "        marker=model_marker,  # Marker determined by model\n",
    "        label=model_name,     # Label only used for handle creation below\n",
    "        alpha=0.6,\n",
    "        s=40                  # Adjust marker size if needed\n",
    "    )\n",
    "    # Create a representative handle for the model legend (marker type)\n",
    "    legend_handles_models.append(plt.Line2D([0], [0], marker=model_marker, color='grey', label=model_name, linestyle='', markersize=8))\n",
    "\n",
    "# --- Add Labels and Legends ---\n",
    "plt.title(f\"t-SNE on Combined CLIP & SigLIP Embeddings (Marker=Model, Color=Size)\", fontsize=18)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=14)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=14)\n",
    "\n",
    "# Legend 1: Models (Markers)\n",
    "legend1 = plt.legend(handles=legend_handles_models, title=\"Model\", loc='upper left', bbox_to_anchor=(1.04, 1), fontsize=10)\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "# Legend 2: Sizes (Colors)\n",
    "size_legend_handles = [plt.scatter([],[], color=size_to_color[sz_label], label=sz_label) for sz_label in unique_size_labels]\n",
    "plt.legend(handles=size_legend_handles, title=\"Size\", loc='center left', bbox_to_anchor=(1.04, 0.5), fontsize=9)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout for external legends\n",
    "\n",
    "# --- Save Plot ---\n",
    "plt.savefig(OUTPUT_PLOT_FILE, dpi=200, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved to {OUTPUT_PLOT_FILE}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nVisualization process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b89503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7798131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c765f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
